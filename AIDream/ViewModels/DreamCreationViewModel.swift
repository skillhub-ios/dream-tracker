import Foundation
import Speech
import CoreData

class DreamCreationViewModel: ObservableObject {
    @Published var dreamContent = ""
    @Published var selectedMood: Mood = .neutral
    @Published var selectedTags: [String] = []
    @Published var interpretation: DreamInterpretation?
    @Published var isRecording = false
    @Published var showingError = false
    @Published var error: Error?
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ru-RU"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    let openAIService = OpenAIService(apiKey: Constants.openAIKey)
    private let context: NSManagedObjectContext
    
    init(context: NSManagedObjectContext = PersistenceController.shared.container.viewContext) {
        self.context = context
    }
    
    func startRecording() {
        guard !isRecording else { return }
        
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    self?.startRecordingSession()
                case .denied, .restricted, .notDetermined:
                    self?.error = NSError(domain: "", code: -1, userInfo: [NSLocalizedDescriptionKey: "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É"])
                    self?.showingError = true
                @unknown default:
                    break
                }
            }
        }
    }
    
    func stopRecording() {
        audioEngine.stop()
        recognitionRequest?.endAudio()
        isRecording = false
    }
    
    private func startRecordingSession() {
        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            error = NSError(domain: "", code: -1, userInfo: [NSLocalizedDescriptionKey: "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"])
            showingError = true
            return
        }
        
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else { return }
            
            recognitionTask = recognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                guard let self = self else { return }
                
                if let result = result {
                    self.dreamContent = result.bestTranscription.formattedString
                }
                
                if error != nil {
                    self.stopRecording()
                }
            }
            
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                self.recognitionRequest?.append(buffer)
            }
            
            audioEngine.prepare()
            try audioEngine.start()
            isRecording = true
            
        } catch {
            self.error = error
            showingError = true
        }
    }
    
    func removeTag(_ tag: String) {
        selectedTags.removeAll { $0 == tag }
    }
    
//    @MainActor
//    func interpretDream() async {
//        do {
//            interpretation = try await openAIService.interpretDream(
//                dream: dreamContent,
//                mood: selectedMood.name,
//                tags: selectedTags
//            )
//        } catch {
//            self.error = error
//            showingError = true
//        }
//    }
    
    @MainActor
    func saveDream(interpretation: DreamInterpretation? = nil) async {
        let dream = DreamEntry(context: context)
        dream.id = UUID()
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–Ω–∞
        print("–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–Ω–∞: '\(dreamContent)'")
        if dreamContent.isEmpty {
            print("–í–ù–ò–ú–ê–ù–ò–ï: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–Ω–∞ –ø—É—Å—Ç–æ–µ!")
        }
        
        dream.content = dreamContent
        dream.date = Date()
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å —ç–º–æ–¥–∑–∏
        let moodWithEmoji = "\(selectedMood.emoji) \(selectedMood.name)"
        print("–°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: \(moodWithEmoji)")
        dream.mood = moodWithEmoji
        
        dream.tags = selectedTags as NSArray
        dream.isSynced = false
        dream.createdAt = Date()
        dream.updatedAt = Date()
        
        if let interpretation = interpretation {
            dream.setInterpretation(interpretation)
        }
        
        do {
            try context.save()
            print("–°–æ–Ω —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: '\(dream.content ?? "")'")
            print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: \(dream.mood ?? "")")
        } catch {
            self.error = error
            showingError = true
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–Ω–∞: \(error)")
        }
    }
}

enum Mood: String, CaseIterable {
    case happy = "–°—á–∞—Å—Ç–ª–∏–≤—ã–π"
    case sad = "–ì—Ä—É—Å—Ç–Ω—ã–π"
    case neutral = "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"
    case anxious = "–¢—Ä–µ–≤–æ–∂–Ω—ã–π"
    case excited = "–í–æ–∑–±—É–∂–¥–µ–Ω–Ω—ã–π"
    
    var emoji: String {
        switch self {
        case .happy: return "üòä"
        case .sad: return "üò¢"
        case .neutral: return "üòê"
        case .anxious: return "üò∞"
        case .excited: return "ü§©"
        }
    }
    
    var name: String { rawValue }
} 
